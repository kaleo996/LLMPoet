# LLMPoet Fine-tuning Config (single file; adjust parameters per compute)

model_path: "./models/Qwen3-8B"
train_data: "./data/training/train.jsonl"
eval_data: "./data/training/eval.jsonl"
output_dir: "./output/finetune"

# Quantization: "4bit" | "8bit" | "none"
quantization: "4bit"

# LoRA: true = LoRA/QLoRA (default); false = full-parameter fine-tuning
use_lora: true

# LoRA params (ignored when use_lora=false)
# rank 32: faster, often sufficient for poetry; 64/128 for heavier adaptation
lora_rank: 32
lora_alpha: 64        # typically rank * 2
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Sequence and batch (poems are short: ~100â€“250 tokens; 384 often sufficient)
max_seq_length: 384
per_device_train_batch_size: 2
per_device_eval_batch_size: 2      # same order as train batch
gradient_accumulation_steps: 8

# Training hyperparameters
num_train_epochs: 3
learning_rate: 0.0002
optim: "adamw_bnb_8bit"            # faster for QLoRA; "adamw_torch" if not using quantization
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.01

# Precision and attention
bf16: true
gradient_checkpointing: true       # saves VRAM
attn_implementation: "sdpa"        # "sdpa" | "flash_attention_2" (faster, needs: pip install flash-attn)

# Distributed (multi-GPU: DeepSpeed config path; single GPU: null)
deepspeed: null

# Logging and checkpointing
logging_steps: 20
eval_strategy: "steps"
eval_steps: 500
save_strategy: "steps"
save_steps: 500
save_total_limit: 3
load_best_model_at_end: true

# Experiment tracking
report_to: "none"                 # "wandb" to log to Weights & Biases

# Other
seed: 42
dataloader_num_workers: 0
preprocessing_num_workers: 4
